# -*- coding: utf-8 -*-
"""
Created on Fri Oct 27 09:28:45 2017

@author: nicolas.berube

This code imports all necessary tools and files to perform queries in collective agreements
The query uses the function retrieve_closest_passages(query,from_pdfs)
    query is the query in string format
    from_pdfs is the list of pdf filenames (without extensions) to perform the query in. = None is in all pdfs
The function returns a nested list of the index of results.
    Each item [n] in the list contains all results from the most relevant collective agreement. Each item contains 3 lists:
    [n][0]: The 1-length list of the most relevant result of the collective agreement
    [n][1]: Every other possible relevent results in the collective agreement (that matches words from the query) in order of relevance
    [n][2]: All other results in the collective agreement -including irrelevant ones-  in order of relevance.

To print the results, is uses the function print_closest_passages(query,answers,num_answers)
    answers is the output -or a subset of the output- of retrieve_closest_passages(query,from_pdfs)
    num_answer (= None for all answers, very long calculation) is the number of displayed answers
The function returns the list of results. Each item [n] of the list contains:
    [n][0] the string of the code of the answer (pdf_name-page_number)
    [n][1] the plain text of the answer
    [n][2] the list of pairs of characters to highlight in the results - from the text [n][1], you need to highlight all characters [n][2][k][0]:[n][2][k][1] for all k in [n][2].
This code needs to have all pickled pdftotext collective agreement in the directory in variable TEXT_DATA_DIR (line 222) generate with the code DownloadData.py for the print_closest_passages(query,answers,num_answers) to work

This codes needs the following files generated by the code GenerateVectors.py in the folder models/:
    idx_to_filename.pkl
    idx_to_metadata.pkl
    allvocab_to_idx.pkl
    wvvocab_to_idx.pkl
    counts.pkl
    embedding_matrix.pkl
    sentence_vectors.pkl
    sentence_words.pkl
    sentence_words_idx.pkl
    english_words.pkl
    french_words.pkl
    pca_component.npy

It also needs the code GenerateVectors.py in the same directory of the current code to import a few functions from it:  filterCA, cleanpassage, sentence_to_vec_idx
"""

import os
import pickle
from datetime import datetime
import numpy as np
import GenerateVectors as GV
from unidecode import unidecode
from collections import OrderedDict
import re
    # For filterCA, cleanpassage, sentence_to_vec_idx

def query_to_vector(text):
    global wvvocab_to_idx, wvvocab
    global counts, embedding_matrix

    sentence_idx = [wvvocab_to_idx[word] for word in GV.cleanpassage(text).split() if word in wvvocab]
    return(GV.sentence_to_vec_idx([sentence_idx],counts,embedding_matrix,from_persisted=True))

def distdot(x,y):
    a = np.empty((len(x),len(y)))
    for i,xi in enumerate(x):
        for j,yj in enumerate(y):
            if np.linalg.norm(xi) == 0 or np.linalg.norm(yj) == 0:
                a[i,j] = -1
            else:
                a[i,j] = np.dot(xi,yj)
    return(-a)

def ngram_positions(ngram,phrase):
    "Retourne la ou les positions des ngrams dans la phrase"
    positions = []
    index = 0
    while index < len(phrase):
        index = phrase.find(ngram, index)
        if index == -1:
            break
        positions.append([index,index+len(ngram)])
        index += 1
    return(positions)

def list_in_array(lis,vec):
    includedflag = False
    for i in np.where(vec == lis[0])[0]:
        if includedflag:
            break
        includedflag = True
        if i + len(lis) - 1 <= len(vec):
            for j,m in enumerate(lis[1:]):
                if vec[i+j+1] != m:
                    includedflag = False
                    break
    return(includedflag)

def fuse_positions(positions):
    fpos = sorted(positions,key=lambda k:k[0])
    i = 1
    while i < len(fpos):
        if fpos[i][0] <= fpos[i-1][1]:
            fpos[i-1][1] = max(fpos[i-1][1],fpos[i][1])
            del fpos[i]
        else:
            i += 1
    return(fpos)

def retrieve_closest_passages(query, from_pdfs=None, idx_true=None):
    "retourne la liste des num_passages les plus proches de la query parmis les sentence_vectors."
    # from_pdf: liste de pdf. Seulement parmis les vectors du code pdf fourni (format idx_to_filename).
    # idx_true: Passage recherché (format int idx de la liste pages_list), si fourni, retourne aussi la position dans les queries de la vraie réponse dans la variable position_true.
    global sentence_vectors, sentence_words, sentence_words_idx
    global idx_to_filename, idx_to_metadata, allvocab_to_idx, allvocab, wvvocab_to_idx, wvvocab
    global counts, embedding_matrix
    global position_true

    global indexes, filtered_vectors, closest_indexes, CAorder, CAdord

    query_idx = []
    for i,s in enumerate(query.split('"')):
        if i%2 == 0:
            query_idx += [[allvocab_to_idx[word]] for word in GV.cleanpassage(s).split() if word in allvocab]
        else:
            query_idx.append([allvocab_to_idx[word] for word in GV.cleanpassage(s).split() if word in allvocab])

    indexes = []
    filtered_vectors = []

    if from_pdfs is not None: pdf_list = set([x.replace('-','') for x in from_pdfs])
    else: pdf_list = set(idx_to_filename.values())

    for i, v in enumerate(sentence_vectors):
        if idx_to_filename[int(idx_to_metadata[i].split("-")[0])] in pdf_list:
            grepsome = 0
            for words_idx in query_idx:
                if list_in_array(words_idx,sentence_words[sentence_words_idx[i]:sentence_words_idx[i+1]]):
                    grepsome += 1
            filtered_vectors.append(v)
            indexes.append([i, len(indexes), grepsome, int(idx_to_metadata[i].split('-')[0])])

    if len(filtered_vectors) == 0:
        print('ERREUR: pdf(s) introuvable:', ' '.join(from_pdfs))
        return([])

    vector = query_to_vector(query)
    distances = distdot([vector], filtered_vectors)[0]


    closest_indexes = sorted(indexes, key=lambda k: (-k[2], distances[k[1]]))
    if idx_true is not None:
        position_true = list(map(list, zip(*closest_indexes)))[0].index(idx_true) + 1

    CAorder = list(OrderedDict.fromkeys(list(map(list, zip(*closest_indexes)))[3]))
    CAdord = {}
    for i,d in enumerate(CAorder):
        CAdord[d] = i
    for i in range(len(indexes)):
        indexes[i][3] = CAdord[indexes[i][3]]

    closest_indexes = sorted(indexes, key=lambda k: (k[3],-k[2], distances[k[1]]))

    answers = []
    ans = [[closest_indexes[0][0]],[],[]]
    for i in range(1,len(closest_indexes)+1):
        if i == len(closest_indexes):
            answers.append(ans)
        elif i != 0 and (closest_indexes[i-1][3] != closest_indexes[i][3]):
            answers.append(ans)
            ans = [[closest_indexes[i][0]],[],[]]
        elif closest_indexes[i][2] != 0:
            ans[1].append(closest_indexes[i][0])
        else:
            ans[2].append(closest_indexes[i][0])

    #answers = list(map(list, zip(*closest_indexes)))[0]

    return answers

def print_closest_passages(query, answers, num_answers=None):
    global idx_to_metadata, idx_to_filename
    global TEXT_DATA_DIR
    global english_words
    global french_words

    if num_answers == None: num_answers = len(answers)

    metalist = [(int(idx_to_metadata[idx].split('-')[0]),int(idx_to_metadata[idx].split('-')[1])) for idx in answers][:num_answers]
    pdflist = list(set(list(map(list, zip(*metalist)))[0]))

    textCA = []
    for pdf in pdflist:
        textCA.append(GV.filterCA(TEXT_DATA_DIR+idx_to_filename[pdf]+'.pkl',english_words,french_words))

    results = []
    for meta in metalist:
        text = '\n'.join(textCA[pdflist.index(meta[0])][meta[1]])
        cleantext = unidecode(text).lower()
        highlight_pos = []
        for word in GV.cleanpassage(query).split():
            highlight_pos += ngram_positions(word,cleantext)
            highlight_pos = fuse_positions(highlight_pos)
        results.append([idx_to_filename[meta[0]]+'-%i'%meta[1],text,highlight_pos])

    urls = []
    for result in results:
        filename = result[0]
        url = "http://negotech.labour.gc.ca/{}/{}/{}/{}.pdf"
        if filename.split("-")[0][-1] == 'a':
            url = url.format("eng", "agreements",filename[:2], filename)
        else:
            url = url.format("fra", "conventions",filename[:2], filename)

        urls.append(url)

    response = [{"metadata":result[0], "raw_passage":result[1],"pdf_url":urls[index],"highlights":result[2]} for index,result in enumerate(results)]
    return response

if __name__ == "__main__":
    print(str(datetime.now())+'\t'+'Importation des fichiers')
    #"""
    idx_to_filename = pickle.load(open("models/idx_to_filename.pkl",'rb'))
    idx_to_metadata = pickle.load(open("models/idx_to_metadata.pkl",'rb'))
    allvocab_to_idx = pickle.load(open("models/allvocab_to_idx.pkl",'rb'))
    allvocab = allvocab_to_idx.keys()
    wvvocab_to_idx = pickle.load(open("models/wvvocab_to_idx.pkl",'rb'))
    wvvocab = wvvocab_to_idx.keys()
    counts = pickle.load(open("models/word_counts.pkl",'rb'))
    embedding_matrix = pickle.load(open("models/embedding_matrix.pkl",'rb'))
    sentence_vectors = pickle.load(open("models/sentence_vectors.pkl",'rb'))
    sentence_words = pickle.load(open("models/sentence_words.pkl",'rb'))
    sentence_words_idx = pickle.load(open("models/sentence_words_idx.pkl",'rb'))
    english_words = pickle.load(open("models/english_words.pkl",'rb')).keys()
    french_words = pickle.load(open("models/french_words.pkl",'rb')).keys()
    #"""
    EMBEDDING_DIM = 100
    cwd = os.getcwd()+'/'
    TEXT_DATA_DIR = cwd+'data/txt-pdftotext-fed/'


query = '"Coffee break"'
x = retrieve_closest_passages(query)
y = print_closest_passages(query,x[0][0])
print(y)
